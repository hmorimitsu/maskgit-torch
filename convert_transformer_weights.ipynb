{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code used to convert the official MaskGIT transformer weights from JAX to PyTorch.\n",
    "\n",
    "If you want to use it, you will have two install two additional requirements: flax and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import tensorflow.compat.v1 as tf\n",
    "import torch\n",
    "\n",
    "from maskgit.nets.bidirectional_transformer import BidirectionalTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_from_path(path):\n",
    "  with tf.io.gfile.GFile(path, \"rb\") as f:\n",
    "    state = flax.serialization.from_bytes(None, f.read())\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(jax_params, ckpt, name):\n",
    "    for k, v in ckpt.items():\n",
    "        if isinstance(v, dict):\n",
    "            get_params(jax_params, v, name + [k])\n",
    "        else:\n",
    "            jax_params['.'.join(name + [k])] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'checkpoints/maskgit_imagenet256_checkpoint'\n",
    "ckpt = restore_from_path(path)['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed_0.embeddings_ln.bias (768,)\n",
      "Embed_0.embeddings_ln.scale (768,)\n",
      "Embed_0.position_embeddings.embedding (257, 768)\n",
      "Embed_0.word_embeddings.embedding (2025, 768)\n",
      "TransformerLayer_0.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_0.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_0.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_0.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_0.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_0.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_0.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_0.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_0.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_0.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_0.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_0.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_0.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_0.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_0.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_0.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_1.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_1.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_1.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_1.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_1.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_1.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_1.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_1.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_1.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_1.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_1.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_1.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_1.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_1.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_1.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_1.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_10.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_10.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_10.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_10.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_10.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_10.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_10.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_10.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_10.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_10.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_10.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_10.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_10.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_10.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_10.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_10.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_11.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_11.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_11.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_11.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_11.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_11.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_11.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_11.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_11.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_11.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_11.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_11.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_11.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_11.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_11.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_11.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_12.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_12.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_12.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_12.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_12.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_12.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_12.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_12.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_12.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_12.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_12.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_12.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_12.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_12.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_12.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_12.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_13.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_13.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_13.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_13.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_13.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_13.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_13.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_13.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_13.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_13.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_13.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_13.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_13.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_13.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_13.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_13.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_14.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_14.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_14.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_14.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_14.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_14.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_14.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_14.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_14.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_14.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_14.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_14.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_14.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_14.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_14.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_14.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_15.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_15.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_15.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_15.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_15.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_15.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_15.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_15.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_15.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_15.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_15.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_15.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_15.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_15.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_15.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_15.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_16.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_16.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_16.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_16.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_16.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_16.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_16.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_16.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_16.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_16.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_16.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_16.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_16.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_16.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_16.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_16.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_17.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_17.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_17.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_17.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_17.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_17.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_17.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_17.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_17.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_17.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_17.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_17.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_17.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_17.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_17.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_17.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_18.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_18.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_18.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_18.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_18.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_18.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_18.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_18.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_18.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_18.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_18.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_18.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_18.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_18.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_18.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_18.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_19.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_19.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_19.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_19.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_19.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_19.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_19.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_19.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_19.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_19.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_19.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_19.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_19.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_19.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_19.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_19.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_2.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_2.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_2.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_2.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_2.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_2.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_2.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_2.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_2.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_2.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_2.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_2.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_2.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_2.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_2.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_2.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_20.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_20.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_20.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_20.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_20.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_20.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_20.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_20.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_20.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_20.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_20.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_20.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_20.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_20.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_20.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_20.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_21.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_21.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_21.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_21.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_21.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_21.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_21.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_21.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_21.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_21.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_21.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_21.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_21.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_21.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_21.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_21.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_22.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_22.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_22.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_22.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_22.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_22.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_22.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_22.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_22.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_22.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_22.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_22.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_22.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_22.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_22.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_22.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_23.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_23.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_23.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_23.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_23.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_23.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_23.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_23.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_23.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_23.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_23.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_23.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_23.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_23.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_23.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_23.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_3.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_3.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_3.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_3.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_3.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_3.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_3.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_3.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_3.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_3.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_3.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_3.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_3.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_3.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_3.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_3.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_4.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_4.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_4.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_4.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_4.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_4.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_4.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_4.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_4.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_4.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_4.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_4.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_4.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_4.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_4.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_4.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_5.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_5.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_5.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_5.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_5.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_5.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_5.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_5.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_5.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_5.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_5.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_5.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_5.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_5.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_5.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_5.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_6.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_6.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_6.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_6.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_6.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_6.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_6.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_6.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_6.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_6.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_6.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_6.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_6.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_6.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_6.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_6.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_7.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_7.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_7.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_7.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_7.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_7.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_7.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_7.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_7.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_7.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_7.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_7.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_7.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_7.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_7.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_7.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_8.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_8.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_8.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_8.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_8.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_8.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_8.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_8.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_8.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_8.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_8.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_8.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_8.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_8.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_8.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_8.Mlp_0.layer_output_ln.scale (768,)\n",
      "TransformerLayer_9.Attention_0.attention_output_ln.bias (768,)\n",
      "TransformerLayer_9.Attention_0.attention_output_ln.scale (768,)\n",
      "TransformerLayer_9.Attention_0.self_attention.key.bias (16, 48)\n",
      "TransformerLayer_9.Attention_0.self_attention.key.kernel (768, 16, 48)\n",
      "TransformerLayer_9.Attention_0.self_attention.out.bias (768,)\n",
      "TransformerLayer_9.Attention_0.self_attention.out.kernel (16, 48, 768)\n",
      "TransformerLayer_9.Attention_0.self_attention.query.bias (16, 48)\n",
      "TransformerLayer_9.Attention_0.self_attention.query.kernel (768, 16, 48)\n",
      "TransformerLayer_9.Attention_0.self_attention.value.bias (16, 48)\n",
      "TransformerLayer_9.Attention_0.self_attention.value.kernel (768, 16, 48)\n",
      "TransformerLayer_9.Mlp_0.intermediate_output.bias (3072,)\n",
      "TransformerLayer_9.Mlp_0.intermediate_output.kernel (768, 3072)\n",
      "TransformerLayer_9.Mlp_0.layer_output.bias (768,)\n",
      "TransformerLayer_9.Mlp_0.layer_output.kernel (3072, 768)\n",
      "TransformerLayer_9.Mlp_0.layer_output_ln.bias (768,)\n",
      "TransformerLayer_9.Mlp_0.layer_output_ln.scale (768,)\n",
      "MlmLayer_0.mlm_bias.bias (2025,)\n",
      "MlmLayer_0.mlm_dense.bias (768,)\n",
      "MlmLayer_0.mlm_dense.kernel (768, 768)\n",
      "MlmLayer_0.mlm_ln.bias (768,)\n",
      "MlmLayer_0.mlm_ln.scale (768,)\n"
     ]
    }
   ],
   "source": [
    "jax_params = {}\n",
    "get_params(jax_params, ckpt, [])\n",
    "for k, v in jax_params.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_emb torch.Size([257, 768])\n",
      "bias torch.Size([2025])\n",
      "tok_emb.weight torch.Size([2025, 768])\n",
      "blocks.0.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.0.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.0.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.0.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.0.AttentionLN.weight torch.Size([768])\n",
      "blocks.0.AttentionLN.bias torch.Size([768])\n",
      "blocks.0.MlpLN.weight torch.Size([768])\n",
      "blocks.0.MlpLN.bias torch.Size([768])\n",
      "blocks.0.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.0.MLP.0.bias torch.Size([3072])\n",
      "blocks.0.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.0.MLP.2.bias torch.Size([768])\n",
      "blocks.1.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.1.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.1.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.1.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.1.AttentionLN.weight torch.Size([768])\n",
      "blocks.1.AttentionLN.bias torch.Size([768])\n",
      "blocks.1.MlpLN.weight torch.Size([768])\n",
      "blocks.1.MlpLN.bias torch.Size([768])\n",
      "blocks.1.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.1.MLP.0.bias torch.Size([3072])\n",
      "blocks.1.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.1.MLP.2.bias torch.Size([768])\n",
      "blocks.2.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.2.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.2.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.2.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.2.AttentionLN.weight torch.Size([768])\n",
      "blocks.2.AttentionLN.bias torch.Size([768])\n",
      "blocks.2.MlpLN.weight torch.Size([768])\n",
      "blocks.2.MlpLN.bias torch.Size([768])\n",
      "blocks.2.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.2.MLP.0.bias torch.Size([3072])\n",
      "blocks.2.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.2.MLP.2.bias torch.Size([768])\n",
      "blocks.3.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.3.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.3.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.3.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.3.AttentionLN.weight torch.Size([768])\n",
      "blocks.3.AttentionLN.bias torch.Size([768])\n",
      "blocks.3.MlpLN.weight torch.Size([768])\n",
      "blocks.3.MlpLN.bias torch.Size([768])\n",
      "blocks.3.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.3.MLP.0.bias torch.Size([3072])\n",
      "blocks.3.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.3.MLP.2.bias torch.Size([768])\n",
      "blocks.4.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.4.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.4.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.4.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.4.AttentionLN.weight torch.Size([768])\n",
      "blocks.4.AttentionLN.bias torch.Size([768])\n",
      "blocks.4.MlpLN.weight torch.Size([768])\n",
      "blocks.4.MlpLN.bias torch.Size([768])\n",
      "blocks.4.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.4.MLP.0.bias torch.Size([3072])\n",
      "blocks.4.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.4.MLP.2.bias torch.Size([768])\n",
      "blocks.5.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.5.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.5.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.5.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.5.AttentionLN.weight torch.Size([768])\n",
      "blocks.5.AttentionLN.bias torch.Size([768])\n",
      "blocks.5.MlpLN.weight torch.Size([768])\n",
      "blocks.5.MlpLN.bias torch.Size([768])\n",
      "blocks.5.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.5.MLP.0.bias torch.Size([3072])\n",
      "blocks.5.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.5.MLP.2.bias torch.Size([768])\n",
      "blocks.6.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.6.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.6.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.6.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.6.AttentionLN.weight torch.Size([768])\n",
      "blocks.6.AttentionLN.bias torch.Size([768])\n",
      "blocks.6.MlpLN.weight torch.Size([768])\n",
      "blocks.6.MlpLN.bias torch.Size([768])\n",
      "blocks.6.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.6.MLP.0.bias torch.Size([3072])\n",
      "blocks.6.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.6.MLP.2.bias torch.Size([768])\n",
      "blocks.7.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.7.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.7.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.7.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.7.AttentionLN.weight torch.Size([768])\n",
      "blocks.7.AttentionLN.bias torch.Size([768])\n",
      "blocks.7.MlpLN.weight torch.Size([768])\n",
      "blocks.7.MlpLN.bias torch.Size([768])\n",
      "blocks.7.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.7.MLP.0.bias torch.Size([3072])\n",
      "blocks.7.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.7.MLP.2.bias torch.Size([768])\n",
      "blocks.8.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.8.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.8.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.8.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.8.AttentionLN.weight torch.Size([768])\n",
      "blocks.8.AttentionLN.bias torch.Size([768])\n",
      "blocks.8.MlpLN.weight torch.Size([768])\n",
      "blocks.8.MlpLN.bias torch.Size([768])\n",
      "blocks.8.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.8.MLP.0.bias torch.Size([3072])\n",
      "blocks.8.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.8.MLP.2.bias torch.Size([768])\n",
      "blocks.9.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.9.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.9.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.9.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.9.AttentionLN.weight torch.Size([768])\n",
      "blocks.9.AttentionLN.bias torch.Size([768])\n",
      "blocks.9.MlpLN.weight torch.Size([768])\n",
      "blocks.9.MlpLN.bias torch.Size([768])\n",
      "blocks.9.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.9.MLP.0.bias torch.Size([3072])\n",
      "blocks.9.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.9.MLP.2.bias torch.Size([768])\n",
      "blocks.10.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.10.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.10.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.10.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.10.AttentionLN.weight torch.Size([768])\n",
      "blocks.10.AttentionLN.bias torch.Size([768])\n",
      "blocks.10.MlpLN.weight torch.Size([768])\n",
      "blocks.10.MlpLN.bias torch.Size([768])\n",
      "blocks.10.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.10.MLP.0.bias torch.Size([3072])\n",
      "blocks.10.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.10.MLP.2.bias torch.Size([768])\n",
      "blocks.11.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.11.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.11.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.11.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.11.AttentionLN.weight torch.Size([768])\n",
      "blocks.11.AttentionLN.bias torch.Size([768])\n",
      "blocks.11.MlpLN.weight torch.Size([768])\n",
      "blocks.11.MlpLN.bias torch.Size([768])\n",
      "blocks.11.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.11.MLP.0.bias torch.Size([3072])\n",
      "blocks.11.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.11.MLP.2.bias torch.Size([768])\n",
      "blocks.12.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.12.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.12.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.12.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.12.AttentionLN.weight torch.Size([768])\n",
      "blocks.12.AttentionLN.bias torch.Size([768])\n",
      "blocks.12.MlpLN.weight torch.Size([768])\n",
      "blocks.12.MlpLN.bias torch.Size([768])\n",
      "blocks.12.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.12.MLP.0.bias torch.Size([3072])\n",
      "blocks.12.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.12.MLP.2.bias torch.Size([768])\n",
      "blocks.13.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.13.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.13.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.13.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.13.AttentionLN.weight torch.Size([768])\n",
      "blocks.13.AttentionLN.bias torch.Size([768])\n",
      "blocks.13.MlpLN.weight torch.Size([768])\n",
      "blocks.13.MlpLN.bias torch.Size([768])\n",
      "blocks.13.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.13.MLP.0.bias torch.Size([3072])\n",
      "blocks.13.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.13.MLP.2.bias torch.Size([768])\n",
      "blocks.14.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.14.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.14.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.14.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.14.AttentionLN.weight torch.Size([768])\n",
      "blocks.14.AttentionLN.bias torch.Size([768])\n",
      "blocks.14.MlpLN.weight torch.Size([768])\n",
      "blocks.14.MlpLN.bias torch.Size([768])\n",
      "blocks.14.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.14.MLP.0.bias torch.Size([3072])\n",
      "blocks.14.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.14.MLP.2.bias torch.Size([768])\n",
      "blocks.15.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.15.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.15.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.15.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.15.AttentionLN.weight torch.Size([768])\n",
      "blocks.15.AttentionLN.bias torch.Size([768])\n",
      "blocks.15.MlpLN.weight torch.Size([768])\n",
      "blocks.15.MlpLN.bias torch.Size([768])\n",
      "blocks.15.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.15.MLP.0.bias torch.Size([3072])\n",
      "blocks.15.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.15.MLP.2.bias torch.Size([768])\n",
      "blocks.16.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.16.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.16.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.16.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.16.AttentionLN.weight torch.Size([768])\n",
      "blocks.16.AttentionLN.bias torch.Size([768])\n",
      "blocks.16.MlpLN.weight torch.Size([768])\n",
      "blocks.16.MlpLN.bias torch.Size([768])\n",
      "blocks.16.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.16.MLP.0.bias torch.Size([3072])\n",
      "blocks.16.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.16.MLP.2.bias torch.Size([768])\n",
      "blocks.17.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.17.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.17.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.17.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.17.AttentionLN.weight torch.Size([768])\n",
      "blocks.17.AttentionLN.bias torch.Size([768])\n",
      "blocks.17.MlpLN.weight torch.Size([768])\n",
      "blocks.17.MlpLN.bias torch.Size([768])\n",
      "blocks.17.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.17.MLP.0.bias torch.Size([3072])\n",
      "blocks.17.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.17.MLP.2.bias torch.Size([768])\n",
      "blocks.18.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.18.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.18.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.18.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.18.AttentionLN.weight torch.Size([768])\n",
      "blocks.18.AttentionLN.bias torch.Size([768])\n",
      "blocks.18.MlpLN.weight torch.Size([768])\n",
      "blocks.18.MlpLN.bias torch.Size([768])\n",
      "blocks.18.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.18.MLP.0.bias torch.Size([3072])\n",
      "blocks.18.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.18.MLP.2.bias torch.Size([768])\n",
      "blocks.19.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.19.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.19.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.19.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.19.AttentionLN.weight torch.Size([768])\n",
      "blocks.19.AttentionLN.bias torch.Size([768])\n",
      "blocks.19.MlpLN.weight torch.Size([768])\n",
      "blocks.19.MlpLN.bias torch.Size([768])\n",
      "blocks.19.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.19.MLP.0.bias torch.Size([3072])\n",
      "blocks.19.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.19.MLP.2.bias torch.Size([768])\n",
      "blocks.20.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.20.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.20.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.20.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.20.AttentionLN.weight torch.Size([768])\n",
      "blocks.20.AttentionLN.bias torch.Size([768])\n",
      "blocks.20.MlpLN.weight torch.Size([768])\n",
      "blocks.20.MlpLN.bias torch.Size([768])\n",
      "blocks.20.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.20.MLP.0.bias torch.Size([3072])\n",
      "blocks.20.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.20.MLP.2.bias torch.Size([768])\n",
      "blocks.21.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.21.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.21.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.21.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.21.AttentionLN.weight torch.Size([768])\n",
      "blocks.21.AttentionLN.bias torch.Size([768])\n",
      "blocks.21.MlpLN.weight torch.Size([768])\n",
      "blocks.21.MlpLN.bias torch.Size([768])\n",
      "blocks.21.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.21.MLP.0.bias torch.Size([3072])\n",
      "blocks.21.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.21.MLP.2.bias torch.Size([768])\n",
      "blocks.22.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.22.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.22.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.22.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.22.AttentionLN.weight torch.Size([768])\n",
      "blocks.22.AttentionLN.bias torch.Size([768])\n",
      "blocks.22.MlpLN.weight torch.Size([768])\n",
      "blocks.22.MlpLN.bias torch.Size([768])\n",
      "blocks.22.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.22.MLP.0.bias torch.Size([3072])\n",
      "blocks.22.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.22.MLP.2.bias torch.Size([768])\n",
      "blocks.23.MultiHeadAttention.in_proj_weight torch.Size([2304, 768])\n",
      "blocks.23.MultiHeadAttention.in_proj_bias torch.Size([2304])\n",
      "blocks.23.MultiHeadAttention.out_proj.weight torch.Size([768, 768])\n",
      "blocks.23.MultiHeadAttention.out_proj.bias torch.Size([768])\n",
      "blocks.23.AttentionLN.weight torch.Size([768])\n",
      "blocks.23.AttentionLN.bias torch.Size([768])\n",
      "blocks.23.MlpLN.weight torch.Size([768])\n",
      "blocks.23.MlpLN.bias torch.Size([768])\n",
      "blocks.23.MLP.0.weight torch.Size([3072, 768])\n",
      "blocks.23.MLP.0.bias torch.Size([3072])\n",
      "blocks.23.MLP.2.weight torch.Size([768, 3072])\n",
      "blocks.23.MLP.2.bias torch.Size([768])\n",
      "Token_Prediction.0.weight torch.Size([768, 768])\n",
      "Token_Prediction.0.bias torch.Size([768])\n",
      "Token_Prediction.2.weight torch.Size([768])\n",
      "Token_Prediction.2.bias torch.Size([768])\n",
      "emb_ln.weight torch.Size([768])\n",
      "emb_ln.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.num_image_tokens = 257\n",
    "        self.num_codebook_vectors = 2025\n",
    "        self.dim = 768\n",
    "        self.n_layers = 24\n",
    "        self.hidden_dim = 3072\n",
    "        self.num_heads = 8\n",
    "        self.attention_dropout = 0.1\n",
    "        self.hidden_dropout = 0.1\n",
    "\n",
    "transformer = BidirectionalTransformer(\n",
    "    num_image_tokens=257,\n",
    "    num_codebook_vectors=2025,\n",
    "    dim=768,\n",
    "    n_layers=24,\n",
    "    hidden_dim=3072,\n",
    "    num_heads=8,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1\n",
    ")\n",
    "for n, p in transformer.named_parameters():\n",
    "    print(n, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_ln.bias torch.Size([768]) tensor(-0.7091) tensor(0.6671)\n",
      "emb_ln.weight torch.Size([768]) tensor(0.4073) tensor(1.9009)\n",
      "pos_emb torch.Size([257, 768]) tensor(-0.2600) tensor(0.4943)\n",
      "tok_emb.weight torch.Size([2025, 768]) tensor(-1.3872) tensor(0.9978)\n",
      "blocks.0.AttentionLN.bias torch.Size([768]) tensor(-2.9947) tensor(2.4949)\n",
      "blocks.0.AttentionLN.weight torch.Size([768]) tensor(0.7839) tensor(1.8952)\n",
      "blocks.0.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.3167) tensor(0.4164)\n",
      "blocks.0.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.4847) tensor(0.3539)\n",
      "blocks.0.MLP.0.bias torch.Size([3072]) tensor(-0.5490) tensor(0.0892)\n",
      "blocks.0.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3889) tensor(0.3238)\n",
      "blocks.0.MLP.2.bias torch.Size([768]) tensor(-0.2196) tensor(0.1671)\n",
      "blocks.0.MLP.2.weight torch.Size([768, 3072]) tensor(-1.1138) tensor(1.0315)\n",
      "blocks.0.MlpLN.bias torch.Size([768]) tensor(-0.9987) tensor(0.4279)\n",
      "blocks.0.MlpLN.weight torch.Size([768]) tensor(0.2591) tensor(1.5465)\n",
      "blocks.1.AttentionLN.bias torch.Size([768]) tensor(-2.9425) tensor(1.1983)\n",
      "blocks.1.AttentionLN.weight torch.Size([768]) tensor(0.8926) tensor(2.9403)\n",
      "blocks.1.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.3945) tensor(0.2343)\n",
      "blocks.1.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.4446) tensor(0.3338)\n",
      "blocks.1.MLP.0.bias torch.Size([3072]) tensor(-0.4904) tensor(0.0499)\n",
      "blocks.1.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2964) tensor(0.2157)\n",
      "blocks.1.MLP.2.bias torch.Size([768]) tensor(-0.2100) tensor(0.2119)\n",
      "blocks.1.MLP.2.weight torch.Size([768, 3072]) tensor(-0.4161) tensor(0.8001)\n",
      "blocks.1.MlpLN.bias torch.Size([768]) tensor(-0.6000) tensor(1.0641)\n",
      "blocks.1.MlpLN.weight torch.Size([768]) tensor(0.1440) tensor(1.3277)\n",
      "blocks.10.AttentionLN.bias torch.Size([768]) tensor(-0.6884) tensor(1.2374)\n",
      "blocks.10.AttentionLN.weight torch.Size([768]) tensor(0.7264) tensor(2.2854)\n",
      "blocks.10.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1371) tensor(0.1329)\n",
      "blocks.10.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1362) tensor(0.1347)\n",
      "blocks.10.MLP.0.bias torch.Size([3072]) tensor(-0.4915) tensor(0.3471)\n",
      "blocks.10.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2289) tensor(0.1698)\n",
      "blocks.10.MLP.2.bias torch.Size([768]) tensor(-0.3882) tensor(0.2166)\n",
      "blocks.10.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3760) tensor(0.3738)\n",
      "blocks.10.MlpLN.bias torch.Size([768]) tensor(-0.5821) tensor(0.5810)\n",
      "blocks.10.MlpLN.weight torch.Size([768]) tensor(0.5880) tensor(1.6718)\n",
      "blocks.11.AttentionLN.bias torch.Size([768]) tensor(-0.7706) tensor(1.0040)\n",
      "blocks.11.AttentionLN.weight torch.Size([768]) tensor(0.7195) tensor(2.1617)\n",
      "blocks.11.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1006) tensor(0.0878)\n",
      "blocks.11.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1396) tensor(0.1375)\n",
      "blocks.11.MLP.0.bias torch.Size([3072]) tensor(-0.5248) tensor(0.4526)\n",
      "blocks.11.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2149) tensor(0.2172)\n",
      "blocks.11.MLP.2.bias torch.Size([768]) tensor(-0.3527) tensor(0.3092)\n",
      "blocks.11.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3239) tensor(0.3229)\n",
      "blocks.11.MlpLN.bias torch.Size([768]) tensor(-0.4807) tensor(0.5318)\n",
      "blocks.11.MlpLN.weight torch.Size([768]) tensor(0.8164) tensor(1.6736)\n",
      "blocks.12.AttentionLN.bias torch.Size([768]) tensor(-0.7943) tensor(1.5144)\n",
      "blocks.12.AttentionLN.weight torch.Size([768]) tensor(0.7453) tensor(2.4622)\n",
      "blocks.12.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1216) tensor(0.0979)\n",
      "blocks.12.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1599) tensor(0.1743)\n",
      "blocks.12.MLP.0.bias torch.Size([3072]) tensor(-0.5991) tensor(0.4701)\n",
      "blocks.12.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2744) tensor(0.2288)\n",
      "blocks.12.MLP.2.bias torch.Size([768]) tensor(-0.7674) tensor(0.3279)\n",
      "blocks.12.MLP.2.weight torch.Size([768, 3072]) tensor(-0.5838) tensor(0.5085)\n",
      "blocks.12.MlpLN.bias torch.Size([768]) tensor(-0.4532) tensor(0.5326)\n",
      "blocks.12.MlpLN.weight torch.Size([768]) tensor(0.6753) tensor(1.6728)\n",
      "blocks.13.AttentionLN.bias torch.Size([768]) tensor(-0.6710) tensor(1.6068)\n",
      "blocks.13.AttentionLN.weight torch.Size([768]) tensor(0.7736) tensor(2.7957)\n",
      "blocks.13.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1694) tensor(0.1654)\n",
      "blocks.13.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1467) tensor(0.1505)\n",
      "blocks.13.MLP.0.bias torch.Size([3072]) tensor(-0.7514) tensor(0.4720)\n",
      "blocks.13.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2391) tensor(0.2162)\n",
      "blocks.13.MLP.2.bias torch.Size([768]) tensor(-0.7947) tensor(0.3023)\n",
      "blocks.13.MLP.2.weight torch.Size([768, 3072]) tensor(-0.9169) tensor(0.7689)\n",
      "blocks.13.MlpLN.bias torch.Size([768]) tensor(-0.4548) tensor(0.4887)\n",
      "blocks.13.MlpLN.weight torch.Size([768]) tensor(0.6006) tensor(1.6125)\n",
      "blocks.14.AttentionLN.bias torch.Size([768]) tensor(-0.5389) tensor(1.2697)\n",
      "blocks.14.AttentionLN.weight torch.Size([768]) tensor(0.7615) tensor(2.6629)\n",
      "blocks.14.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.2484) tensor(0.1700)\n",
      "blocks.14.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1377) tensor(0.1492)\n",
      "blocks.14.MLP.0.bias torch.Size([3072]) tensor(-0.7730) tensor(0.7271)\n",
      "blocks.14.MLP.0.weight torch.Size([3072, 768]) tensor(-0.5288) tensor(0.4226)\n",
      "blocks.14.MLP.2.bias torch.Size([768]) tensor(-0.6455) tensor(0.3900)\n",
      "blocks.14.MLP.2.weight torch.Size([768, 3072]) tensor(-1.6698) tensor(0.6436)\n",
      "blocks.14.MlpLN.bias torch.Size([768]) tensor(-0.4016) tensor(0.4420)\n",
      "blocks.14.MlpLN.weight torch.Size([768]) tensor(0.6889) tensor(1.6177)\n",
      "blocks.15.AttentionLN.bias torch.Size([768]) tensor(-0.7686) tensor(1.4117)\n",
      "blocks.15.AttentionLN.weight torch.Size([768]) tensor(0.7701) tensor(2.7973)\n",
      "blocks.15.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1872) tensor(0.1870)\n",
      "blocks.15.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1612) tensor(0.1401)\n",
      "blocks.15.MLP.0.bias torch.Size([3072]) tensor(-0.6797) tensor(0.3919)\n",
      "blocks.15.MLP.0.weight torch.Size([3072, 768]) tensor(-0.4009) tensor(0.3566)\n",
      "blocks.15.MLP.2.bias torch.Size([768]) tensor(-0.6932) tensor(0.2989)\n",
      "blocks.15.MLP.2.weight torch.Size([768, 3072]) tensor(-2.1945) tensor(0.6075)\n",
      "blocks.15.MlpLN.bias torch.Size([768]) tensor(-0.4027) tensor(0.4840)\n",
      "blocks.15.MlpLN.weight torch.Size([768]) tensor(0.6154) tensor(1.5874)\n",
      "blocks.16.AttentionLN.bias torch.Size([768]) tensor(-0.6113) tensor(1.0477)\n",
      "blocks.16.AttentionLN.weight torch.Size([768]) tensor(0.8257) tensor(2.6394)\n",
      "blocks.16.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.2642) tensor(0.2361)\n",
      "blocks.16.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1508) tensor(0.1720)\n",
      "blocks.16.MLP.0.bias torch.Size([3072]) tensor(-0.7346) tensor(0.3239)\n",
      "blocks.16.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3198) tensor(0.4040)\n",
      "blocks.16.MLP.2.bias torch.Size([768]) tensor(-0.8279) tensor(0.2819)\n",
      "blocks.16.MLP.2.weight torch.Size([768, 3072]) tensor(-2.0213) tensor(0.6142)\n",
      "blocks.16.MlpLN.bias torch.Size([768]) tensor(-0.4026) tensor(0.4000)\n",
      "blocks.16.MlpLN.weight torch.Size([768]) tensor(0.5862) tensor(1.6127)\n",
      "blocks.17.AttentionLN.bias torch.Size([768]) tensor(-0.5271) tensor(0.7703)\n",
      "blocks.17.AttentionLN.weight torch.Size([768]) tensor(0.8242) tensor(2.1533)\n",
      "blocks.17.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.3010) tensor(0.2663)\n",
      "blocks.17.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1542) tensor(0.1380)\n",
      "blocks.17.MLP.0.bias torch.Size([3072]) tensor(-0.7905) tensor(0.1979)\n",
      "blocks.17.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3282) tensor(0.4267)\n",
      "blocks.17.MLP.2.bias torch.Size([768]) tensor(-0.4398) tensor(0.3428)\n",
      "blocks.17.MLP.2.weight torch.Size([768, 3072]) tensor(-1.7373) tensor(0.3920)\n",
      "blocks.17.MlpLN.bias torch.Size([768]) tensor(-0.3421) tensor(0.4547)\n",
      "blocks.17.MlpLN.weight torch.Size([768]) tensor(0.7728) tensor(1.6681)\n",
      "blocks.18.AttentionLN.bias torch.Size([768]) tensor(-0.8276) tensor(1.1460)\n",
      "blocks.18.AttentionLN.weight torch.Size([768]) tensor(0.8122) tensor(2.5033)\n",
      "blocks.18.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.2300) tensor(0.2283)\n",
      "blocks.18.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1493) tensor(0.1571)\n",
      "blocks.18.MLP.0.bias torch.Size([3072]) tensor(-0.6654) tensor(0.3636)\n",
      "blocks.18.MLP.0.weight torch.Size([3072, 768]) tensor(-0.4621) tensor(0.3729)\n",
      "blocks.18.MLP.2.bias torch.Size([768]) tensor(-0.3968) tensor(0.3223)\n",
      "blocks.18.MLP.2.weight torch.Size([768, 3072]) tensor(-1.9628) tensor(0.7642)\n",
      "blocks.18.MlpLN.bias torch.Size([768]) tensor(-0.3831) tensor(0.4383)\n",
      "blocks.18.MlpLN.weight torch.Size([768]) tensor(0.6457) tensor(1.5840)\n",
      "blocks.19.AttentionLN.bias torch.Size([768]) tensor(-0.8000) tensor(1.2513)\n",
      "blocks.19.AttentionLN.weight torch.Size([768]) tensor(0.7956) tensor(2.6600)\n",
      "blocks.19.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.3261) tensor(0.3163)\n",
      "blocks.19.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1719) tensor(0.1604)\n",
      "blocks.19.MLP.0.bias torch.Size([3072]) tensor(-0.8362) tensor(0.3421)\n",
      "blocks.19.MLP.0.weight torch.Size([3072, 768]) tensor(-0.4573) tensor(0.4611)\n",
      "blocks.19.MLP.2.bias torch.Size([768]) tensor(-0.4018) tensor(0.4016)\n",
      "blocks.19.MLP.2.weight torch.Size([768, 3072]) tensor(-1.5329) tensor(0.6577)\n",
      "blocks.19.MlpLN.bias torch.Size([768]) tensor(-0.3804) tensor(0.3989)\n",
      "blocks.19.MlpLN.weight torch.Size([768]) tensor(0.6307) tensor(1.6849)\n",
      "blocks.2.AttentionLN.bias torch.Size([768]) tensor(-1.9607) tensor(1.0100)\n",
      "blocks.2.AttentionLN.weight torch.Size([768]) tensor(0.7889) tensor(3.0020)\n",
      "blocks.2.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.4504) tensor(0.1857)\n",
      "blocks.2.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1089) tensor(0.1130)\n",
      "blocks.2.MLP.0.bias torch.Size([3072]) tensor(-0.4566) tensor(0.2545)\n",
      "blocks.2.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3206) tensor(0.1746)\n",
      "blocks.2.MLP.2.bias torch.Size([768]) tensor(-0.6371) tensor(0.2172)\n",
      "blocks.2.MLP.2.weight torch.Size([768, 3072]) tensor(-0.4528) tensor(0.7291)\n",
      "blocks.2.MlpLN.bias torch.Size([768]) tensor(-0.8794) tensor(0.8386)\n",
      "blocks.2.MlpLN.weight torch.Size([768]) tensor(0.2734) tensor(1.2857)\n",
      "blocks.20.AttentionLN.bias torch.Size([768]) tensor(-0.6804) tensor(1.3621)\n",
      "blocks.20.AttentionLN.weight torch.Size([768]) tensor(0.7905) tensor(2.9210)\n",
      "blocks.20.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.4546) tensor(0.3476)\n",
      "blocks.20.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1604) tensor(0.1443)\n",
      "blocks.20.MLP.0.bias torch.Size([3072]) tensor(-0.7963) tensor(0.2107)\n",
      "blocks.20.MLP.0.weight torch.Size([3072, 768]) tensor(-0.4280) tensor(0.3694)\n",
      "blocks.20.MLP.2.bias torch.Size([768]) tensor(-0.2954) tensor(0.2917)\n",
      "blocks.20.MLP.2.weight torch.Size([768, 3072]) tensor(-1.5089) tensor(0.7019)\n",
      "blocks.20.MlpLN.bias torch.Size([768]) tensor(-0.3097) tensor(0.3546)\n",
      "blocks.20.MlpLN.weight torch.Size([768]) tensor(0.5138) tensor(1.6560)\n",
      "blocks.21.AttentionLN.bias torch.Size([768]) tensor(-0.5727) tensor(1.0719)\n",
      "blocks.21.AttentionLN.weight torch.Size([768]) tensor(0.7539) tensor(2.9045)\n",
      "blocks.21.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.4927) tensor(0.4047)\n",
      "blocks.21.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1504) tensor(0.1574)\n",
      "blocks.21.MLP.0.bias torch.Size([3072]) tensor(-0.7412) tensor(0.2175)\n",
      "blocks.21.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2428) tensor(0.1969)\n",
      "blocks.21.MLP.2.bias torch.Size([768]) tensor(-0.2511) tensor(0.3331)\n",
      "blocks.21.MLP.2.weight torch.Size([768, 3072]) tensor(-1.2400) tensor(0.7116)\n",
      "blocks.21.MlpLN.bias torch.Size([768]) tensor(-0.2383) tensor(0.3718)\n",
      "blocks.21.MlpLN.weight torch.Size([768]) tensor(0.5699) tensor(1.6834)\n",
      "blocks.22.AttentionLN.bias torch.Size([768]) tensor(-0.4903) tensor(0.7507)\n",
      "blocks.22.AttentionLN.weight torch.Size([768]) tensor(0.6775) tensor(2.9498)\n",
      "blocks.22.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.5031) tensor(0.5516)\n",
      "blocks.22.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1483) tensor(0.1499)\n",
      "blocks.22.MLP.0.bias torch.Size([3072]) tensor(-0.5156) tensor(0.2594)\n",
      "blocks.22.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3085) tensor(0.3248)\n",
      "blocks.22.MLP.2.bias torch.Size([768]) tensor(-0.4394) tensor(0.3064)\n",
      "blocks.22.MLP.2.weight torch.Size([768, 3072]) tensor(-1.5415) tensor(0.8369)\n",
      "blocks.22.MlpLN.bias torch.Size([768]) tensor(-0.8415) tensor(0.6911)\n",
      "blocks.22.MlpLN.weight torch.Size([768]) tensor(0.2639) tensor(1.7031)\n",
      "blocks.23.AttentionLN.bias torch.Size([768]) tensor(-0.6292) tensor(0.5176)\n",
      "blocks.23.AttentionLN.weight torch.Size([768]) tensor(0.4140) tensor(2.0734)\n",
      "blocks.23.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.6205) tensor(0.4887)\n",
      "blocks.23.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1934) tensor(0.2223)\n",
      "blocks.23.MLP.0.bias torch.Size([3072]) tensor(-0.9301) tensor(0.4103)\n",
      "blocks.23.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3391) tensor(0.2651)\n",
      "blocks.23.MLP.2.bias torch.Size([768]) tensor(-0.1963) tensor(0.3281)\n",
      "blocks.23.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3059) tensor(0.3092)\n",
      "blocks.23.MlpLN.bias torch.Size([768]) tensor(-0.6097) tensor(0.3296)\n",
      "blocks.23.MlpLN.weight torch.Size([768]) tensor(0.5722) tensor(1.5069)\n",
      "blocks.3.AttentionLN.bias torch.Size([768]) tensor(-0.8334) tensor(0.9702)\n",
      "blocks.3.AttentionLN.weight torch.Size([768]) tensor(0.8743) tensor(1.5174)\n",
      "blocks.3.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1233) tensor(0.1864)\n",
      "blocks.3.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1190) tensor(0.1115)\n",
      "blocks.3.MLP.0.bias torch.Size([3072]) tensor(-0.4266) tensor(0.1462)\n",
      "blocks.3.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3921) tensor(0.1758)\n",
      "blocks.3.MLP.2.bias torch.Size([768]) tensor(-0.2169) tensor(0.2095)\n",
      "blocks.3.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3159) tensor(0.7000)\n",
      "blocks.3.MlpLN.bias torch.Size([768]) tensor(-1.1195) tensor(0.5627)\n",
      "blocks.3.MlpLN.weight torch.Size([768]) tensor(0.7946) tensor(1.2772)\n",
      "blocks.4.AttentionLN.bias torch.Size([768]) tensor(-0.7693) tensor(1.3456)\n",
      "blocks.4.AttentionLN.weight torch.Size([768]) tensor(0.9279) tensor(2.1714)\n",
      "blocks.4.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1076) tensor(0.1548)\n",
      "blocks.4.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1545) tensor(0.1116)\n",
      "blocks.4.MLP.0.bias torch.Size([3072]) tensor(-0.5865) tensor(0.3012)\n",
      "blocks.4.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2874) tensor(0.1733)\n",
      "blocks.4.MLP.2.bias torch.Size([768]) tensor(-0.2275) tensor(0.1994)\n",
      "blocks.4.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3283) tensor(0.6965)\n",
      "blocks.4.MlpLN.bias torch.Size([768]) tensor(-0.9471) tensor(0.5634)\n",
      "blocks.4.MlpLN.weight torch.Size([768]) tensor(0.7290) tensor(1.2612)\n",
      "blocks.5.AttentionLN.bias torch.Size([768]) tensor(-0.8280) tensor(1.8698)\n",
      "blocks.5.AttentionLN.weight torch.Size([768]) tensor(0.9030) tensor(2.5846)\n",
      "blocks.5.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.0633) tensor(0.0812)\n",
      "blocks.5.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1195) tensor(0.1260)\n",
      "blocks.5.MLP.0.bias torch.Size([3072]) tensor(-0.3991) tensor(0.1989)\n",
      "blocks.5.MLP.0.weight torch.Size([3072, 768]) tensor(-0.2232) tensor(0.1464)\n",
      "blocks.5.MLP.2.bias torch.Size([768]) tensor(-0.2390) tensor(0.2086)\n",
      "blocks.5.MLP.2.weight torch.Size([768, 3072]) tensor(-0.2666) tensor(0.6071)\n",
      "blocks.5.MlpLN.bias torch.Size([768]) tensor(-0.5391) tensor(0.5183)\n",
      "blocks.5.MlpLN.weight torch.Size([768]) tensor(0.5732) tensor(1.3214)\n",
      "blocks.6.AttentionLN.bias torch.Size([768]) tensor(-0.8036) tensor(1.9613)\n",
      "blocks.6.AttentionLN.weight torch.Size([768]) tensor(0.9343) tensor(2.8727)\n",
      "blocks.6.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.0801) tensor(0.1011)\n",
      "blocks.6.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1256) tensor(0.1138)\n",
      "blocks.6.MLP.0.bias torch.Size([3072]) tensor(-0.4658) tensor(0.3283)\n",
      "blocks.6.MLP.0.weight torch.Size([3072, 768]) tensor(-0.1908) tensor(0.1678)\n",
      "blocks.6.MLP.2.bias torch.Size([768]) tensor(-0.1919) tensor(0.2224)\n",
      "blocks.6.MLP.2.weight torch.Size([768, 3072]) tensor(-0.2671) tensor(0.4160)\n",
      "blocks.6.MlpLN.bias torch.Size([768]) tensor(-0.6169) tensor(0.5447)\n",
      "blocks.6.MlpLN.weight torch.Size([768]) tensor(0.5068) tensor(1.3266)\n",
      "blocks.7.AttentionLN.bias torch.Size([768]) tensor(-0.8320) tensor(1.8322)\n",
      "blocks.7.AttentionLN.weight torch.Size([768]) tensor(0.8789) tensor(3.0199)\n",
      "blocks.7.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1015) tensor(0.0884)\n",
      "blocks.7.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1226) tensor(0.1318)\n",
      "blocks.7.MLP.0.bias torch.Size([3072]) tensor(-0.4684) tensor(0.3522)\n",
      "blocks.7.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3451) tensor(0.1663)\n",
      "blocks.7.MLP.2.bias torch.Size([768]) tensor(-0.2035) tensor(0.2481)\n",
      "blocks.7.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3549) tensor(0.5859)\n",
      "blocks.7.MlpLN.bias torch.Size([768]) tensor(-0.7888) tensor(0.5666)\n",
      "blocks.7.MlpLN.weight torch.Size([768]) tensor(0.4794) tensor(1.4106)\n",
      "blocks.8.AttentionLN.bias torch.Size([768]) tensor(-0.8344) tensor(1.6062)\n",
      "blocks.8.AttentionLN.weight torch.Size([768]) tensor(0.8607) tensor(2.6027)\n",
      "blocks.8.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.0887) tensor(0.1340)\n",
      "blocks.8.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1243) tensor(0.1292)\n",
      "blocks.8.MLP.0.bias torch.Size([3072]) tensor(-0.5341) tensor(0.3265)\n",
      "blocks.8.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3291) tensor(0.1646)\n",
      "blocks.8.MLP.2.bias torch.Size([768]) tensor(-0.2645) tensor(0.2605)\n",
      "blocks.8.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3798) tensor(0.5762)\n",
      "blocks.8.MlpLN.bias torch.Size([768]) tensor(-1.3214) tensor(0.6982)\n",
      "blocks.8.MlpLN.weight torch.Size([768]) tensor(0.3874) tensor(1.4234)\n",
      "blocks.9.AttentionLN.bias torch.Size([768]) tensor(-0.7615) tensor(1.2138)\n",
      "blocks.9.AttentionLN.weight torch.Size([768]) tensor(0.8237) tensor(2.1419)\n",
      "blocks.9.MultiHeadAttention.out_proj.bias torch.Size([768]) tensor(-0.1648) tensor(0.1341)\n",
      "blocks.9.MultiHeadAttention.out_proj.weight torch.Size([768, 768]) tensor(-0.1306) tensor(0.1290)\n",
      "blocks.9.MLP.0.bias torch.Size([3072]) tensor(-0.4651) tensor(0.2925)\n",
      "blocks.9.MLP.0.weight torch.Size([3072, 768]) tensor(-0.3136) tensor(0.1810)\n",
      "blocks.9.MLP.2.bias torch.Size([768]) tensor(-0.3100) tensor(0.2075)\n",
      "blocks.9.MLP.2.weight torch.Size([768, 3072]) tensor(-0.3633) tensor(0.6368)\n",
      "blocks.9.MlpLN.bias torch.Size([768]) tensor(-0.8856) tensor(0.5747)\n",
      "blocks.9.MlpLN.weight torch.Size([768]) tensor(0.4252) tensor(1.5270)\n",
      "bias torch.Size([2025]) tensor(-0.7683) tensor(0.8542)\n",
      "Token_Prediction.0.bias torch.Size([768]) tensor(-0.5584) tensor(1.2792)\n",
      "Token_Prediction.0.weight torch.Size([768, 768]) tensor(-0.2173) tensor(0.2725)\n",
      "Token_Prediction.2.bias torch.Size([768]) tensor(-1.0481) tensor(0.5893)\n",
      "Token_Prediction.2.weight torch.Size([768]) tensor(0.0220) tensor(2.5473)\n",
      "blocks.0.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.6954) tensor(1.5418)\n",
      "blocks.0.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.6861) tensor(0.6819)\n",
      "blocks.1.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.8593) tensor(0.7366)\n",
      "blocks.1.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1942) tensor(0.2113)\n",
      "blocks.2.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.7167) tensor(0.6931)\n",
      "blocks.2.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1618) tensor(0.1758)\n",
      "blocks.3.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.9255) tensor(0.7547)\n",
      "blocks.3.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1807) tensor(0.1868)\n",
      "blocks.4.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.8812) tensor(1.1110)\n",
      "blocks.4.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1816) tensor(0.2198)\n",
      "blocks.5.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.0850) tensor(1.1709)\n",
      "blocks.5.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1502) tensor(0.1431)\n",
      "blocks.6.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2081) tensor(1.1955)\n",
      "blocks.6.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1511) tensor(0.1709)\n",
      "blocks.7.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2952) tensor(1.2037)\n",
      "blocks.7.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1710) tensor(0.1600)\n",
      "blocks.8.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2250) tensor(1.2184)\n",
      "blocks.8.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1599) tensor(0.1601)\n",
      "blocks.9.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.0884) tensor(0.7536)\n",
      "blocks.9.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1800) tensor(0.1761)\n",
      "blocks.10.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.1353) tensor(1.1345)\n",
      "blocks.10.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2023) tensor(0.2109)\n",
      "blocks.11.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2478) tensor(0.9702)\n",
      "blocks.11.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.1792) tensor(0.1619)\n",
      "blocks.12.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.5994) tensor(1.1179)\n",
      "blocks.12.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.3464) tensor(0.4041)\n",
      "blocks.13.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.0567) tensor(0.9339)\n",
      "blocks.13.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2017) tensor(0.1986)\n",
      "blocks.14.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.8098) tensor(0.8178)\n",
      "blocks.14.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2015) tensor(0.1900)\n",
      "blocks.15.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.9894) tensor(0.9409)\n",
      "blocks.15.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2177) tensor(0.2056)\n",
      "blocks.16.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-0.9344) tensor(0.7536)\n",
      "blocks.16.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2662) tensor(0.2228)\n",
      "blocks.17.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.0034) tensor(0.9083)\n",
      "blocks.17.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2864) tensor(0.2573)\n",
      "blocks.18.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2585) tensor(1.0685)\n",
      "blocks.18.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.2192) tensor(0.2170)\n",
      "blocks.19.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.1746) tensor(1.6530)\n",
      "blocks.19.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.3422) tensor(0.3043)\n",
      "blocks.20.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.1778) tensor(1.5981)\n",
      "blocks.20.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.3114) tensor(0.3115)\n",
      "blocks.21.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2342) tensor(0.9513)\n",
      "blocks.21.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.3660) tensor(0.3209)\n",
      "blocks.22.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.2364) tensor(1.5353)\n",
      "blocks.22.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.3234) tensor(0.3254)\n",
      "blocks.23.MultiHeadAttention.in_proj_bias torch.Size([2304]) tensor(-1.5163) tensor(1.0403)\n",
      "blocks.23.MultiHeadAttention.in_proj_weight torch.Size([2304, 768]) tensor(-0.3545) tensor(0.3680)\n"
     ]
    }
   ],
   "source": [
    "torch_params = [[k, torch.from_numpy(v.copy())] for k, v in jax_params.items()]\n",
    "\n",
    "convert_names = {\n",
    "    '.kernel': '.weight',\n",
    "    '.scale': '.weight',\n",
    "    'Embed_0.position_embeddings.embedding': 'pos_emb',\n",
    "    'Embed_0.word_embeddings.embedding': 'tok_emb.weight',\n",
    "    'Embed_0.embeddings_ln': 'emb_ln',\n",
    "    'MlmLayer_0.mlm_bias.bias': 'bias',\n",
    "    'MlmLayer_0.mlm_dense': 'Token_Prediction.0',\n",
    "    'MlmLayer_0.mlm_ln': 'Token_Prediction.2',\n",
    "    'TransformerLayer_': 'blocks.',\n",
    "    'Mlp_': 'MLP.',\n",
    "    'MLP.0.layer_output_ln': 'MlpLN',\n",
    "    '0.intermediate_output': '0',\n",
    "    '0.layer_output': '2',\n",
    "    'Attention_0.attention_output_ln': 'AttentionLN',\n",
    "    'Attention_0.self_attention': 'MultiHeadAttention',\n",
    "    'MultiHeadAttention.out': 'MultiHeadAttention.out_proj',\n",
    "}\n",
    "for in_pat, out_pat in convert_names.items():\n",
    "    for i in range(len(torch_params)):\n",
    "        torch_params[i][0] = torch_params[i][0].replace(in_pat, out_pat)\n",
    "\n",
    "for i in range(len(torch_params)):\n",
    "    name = torch_params[i][0]\n",
    "\n",
    "    if name.startswith('block') and len(torch_params[i][1].shape) == 3:\n",
    "        if torch_params[i][1].shape[0] == 768:\n",
    "            torch_params[i][1] = torch_params[i][1].permute(1, 2, 0)\n",
    "        elif torch_params[i][1].shape[-1] == 768:\n",
    "            torch_params[i][1] = torch_params[i][1].permute(2, 0, 1)\n",
    "    if name.startswith('block') and len(torch_params[i][1].shape) == 2:\n",
    "        if not 'bias' in name:\n",
    "            torch_params[i][1] = torch_params[i][1].permute(1, 0)\n",
    "    if name.startswith('Token_Prediction') and len(torch_params[i][1].shape) == 2:\n",
    "        torch_params[i][1] = torch_params[i][1].permute(1, 0)\n",
    "\n",
    "    if name.startswith('block') and 'bias' in name and len(torch_params[i][1].shape) == 2:\n",
    "        torch_params[i][1] = torch_params[i][1].reshape(-1)\n",
    "    if name.startswith('block') and 'weight' in name and len(torch_params[i][1].shape) == 3:\n",
    "        if torch_params[i][1].shape[0] == 768:\n",
    "            torch_params[i][1] = torch_params[i][1].reshape(torch_params[i][1].shape[0], -1)\n",
    "        elif torch_params[i][1].shape[-1] == 768:\n",
    "            torch_params[i][1] = torch_params[i][1].reshape(-1, torch_params[i][1].shape[-1])\n",
    "\n",
    "cat_torch_params = []\n",
    "for i in range(24):\n",
    "    base_name = f'blocks.{i}.MultiHeadAttention'\n",
    "    for w in ['bias', 'weight']:\n",
    "        params = []\n",
    "        for n in ['query', 'key', 'value']:\n",
    "            name = f'{base_name}.{n}.{w}'\n",
    "            params.append([p[1] for p in torch_params if p[0] == name][0])\n",
    "        out_name = f'{base_name}.in_proj_{w}'\n",
    "        cat_torch_params.append((out_name, torch.cat(params, dim=0)))\n",
    "\n",
    "final_torch_params = []\n",
    "for p in torch_params:\n",
    "    if 'key' in p[0] or 'query' in p[0] or 'value' in p[0]:\n",
    "        continue\n",
    "    final_torch_params.append(p)\n",
    "for p in cat_torch_params:\n",
    "    final_torch_params.append(p)\n",
    "\n",
    "for n, p in final_torch_params:\n",
    "    print(n, p.shape, p.min(), p.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {n: p for n, p in final_torch_params}\n",
    "torch.save({'state_dict': state_dict}, 'checkpoints/maskgit_imagenet256.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('mgtorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e21cd877fcfaa10beb13defb9bedc30d2b197cb1ab7a74a2d25f8d7c307578"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
